2026-02-03 22:04:12 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:219] - Use pytorch device_name: cpu
2026-02-03 22:04:12 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:227] - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-02-03 22:04:13 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:13 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:04:13 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:13 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:04:13 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:14 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-03 22:04:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-03 22:04:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:04:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:04:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-03 22:04:16 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-03 22:04:16 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-03 22:04:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:04:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-03 22:04:17 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #1 for session comprehensive_test_session
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 260
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:17 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:19 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:19 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:19 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #2 for session comprehensive_test_session
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 368
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:04:23 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:25 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:25 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:25 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #3 for session comprehensive_test_session
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 484
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:04:29 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:30 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:30 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:30 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #4 for session comprehensive_test_session
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 521
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:34 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:35 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:35 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:35 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #5 for session comprehensive_test_session
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 555
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:04:38 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:40 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:40 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:40 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #6 for session comprehensive_test_session
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 589
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:04:43 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:45 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:45 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:45 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #7 for session comprehensive_test_session
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 681
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:48 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:50 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:50 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:50 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #8 for session comprehensive_test_session
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 717
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:53 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:55 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:55 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:55 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #9 for session comprehensive_test_session
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 755
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:04:57 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:04:59 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:04:59 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:04:59 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #10 for session comprehensive_test_session
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 813
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:05:02 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:04 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:04 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:04 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #11 for session comprehensive_test_session
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 921
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:08 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:09 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:09 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:09 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #12 for session comprehensive_test_session
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 984
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:13 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:14 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:14 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:14 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #13 for session comprehensive_test_session
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1021
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:05:17 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:19 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:19 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:19 - app.core.pipeline - INFO - [pipeline.py:329] - [Pipeline] Query refined: How do we optimize This for mobile devices?
2026-02-03 22:05:19 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #14 for session comprehensive_test_session
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1114
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:193] - [Pipeline] Spelling corrected: Can we reduce there size significantly?
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:23 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:24 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:24 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:24 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #15 for session comprehensive_test_session
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1147
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:05:27 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:29 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:29 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:29 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #16 for session comprehensive_test_session
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1244
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:05:34 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:35 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:35 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:35 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:05:39 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:05:39 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #17 for session comprehensive_test_session
2026-02-03 22:05:39 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1302
2026-02-03 22:05:39 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:39 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:39 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:210] - [Pipeline] Ambiguity: True (LLM used, confidence: 0.7)
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:05:40 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:42 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:42 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:42 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #18 for session comprehensive_test_session
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1421
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:46 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:48 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:48 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:48 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #19 for session comprehensive_test_session
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1477
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:193] - [Pipeline] Spelling corrected: Given all these constraints, what would be you're recommendation?
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:51 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:53 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:53 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:53 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #20 for session comprehensive_test_session
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 1592
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:05:57 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:05:59 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:05:59 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:05:59 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:06:04 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:06:04 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:219] - Use pytorch device_name: cpu
2026-02-03 22:06:04 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:227] - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-02-03 22:06:04 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:04 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:05 - huggingface_hub.utils._http - WARNING - [_http.py:779] - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-03 22:06:05 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:06 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:06:06 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:06 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:06 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-03 22:06:06 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:06 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:07 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:07 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:07 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:07 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:08 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-03 22:06:08 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-03 22:06:08 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:08 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-03 22:06:09 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #1 for session ambiguity_test_1
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 10
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:09 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #2 for session ambiguity_test_2
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 10
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:13 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #3 for session ambiguity_test_3
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 11
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:16 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #4 for session ambiguity_test_4
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 11
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:19 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #5 for session ambiguity_test_5
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 19
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:23 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:06:24 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:06:24 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:219] - Use pytorch device_name: cpu
2026-02-03 22:06:24 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:227] - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-02-03 22:06:25 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:25 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:06:25 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:25 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:06:25 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:25 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:06:26 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:26 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-03 22:06:26 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:26 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:06:26 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:26 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:27 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-03 22:06:27 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:27 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:27 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:27 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:28 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:28 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:28 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-03 22:06:28 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-03 22:06:29 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:29 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-03 22:06:29 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #1 for session context_test_session
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 13
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:29 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #2 for session context_test_session
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 113
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:33 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #3 for session context_test_session
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 230
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:37 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #4 for session context_test_session
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 331
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:42 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:44 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:06:44 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:44 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:06:48 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:06:48 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:219] - Use pytorch device_name: cpu
2026-02-03 22:06:48 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:227] - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-02-03 22:06:48 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:48 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:06:48 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:48 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:06:49 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:49 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:06:49 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:49 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:50 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:51 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:51 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:51 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:51 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-03 22:06:51 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-03 22:06:52 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-03 22:06:52 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:06:52 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-03 22:06:52 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #1 for session pronoun_test_session
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 16
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:52 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #2 for session pronoun_test_session
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 128
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:329] - [Pipeline] Query refined: How can I optimize I'd for performance?
2026-02-03 22:06:57 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #3 for session pronoun_test_session
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 238
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:01 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #4 for session pronoun_test_session
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 273
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:04 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:06 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:06 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:06 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #5 for session pronoun_test_session
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 367
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:07:10 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:11 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:11 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:11 - app.core.pipeline - INFO - [pipeline.py:329] - [Pipeline] Query refined: Should we deploy I'd on AWS or GCP?
2026-02-03 22:07:11 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:07:15 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:07:15 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:219] - Use pytorch device_name: cpu
2026-02-03 22:07:15 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:227] - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-02-03 22:07:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:15 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:07:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:07:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:07:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:16 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-03 22:07:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:07:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-03 22:07:17 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:18 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:18 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:18 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:18 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:18 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:19 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-03 22:07:19 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-03 22:07:19 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:19 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-03 22:07:20 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #1 for session tracking_test_session
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 14
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:20 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #2 for session tracking_test_session
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 112
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:329] - [Pipeline] Query refined: What does I'd mean?
2026-02-03 22:07:23 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #3 for session tracking_test_session
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 210
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:27 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #4 for session tracking_test_session
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 261
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:31 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:32 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:32 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:32 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #5 for session tracking_test_session
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 296
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:35 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:37 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:37 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:37 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #6 for session tracking_test_session
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 348
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:07:41 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:42 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:42 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:42 - app.core.pipeline - INFO - [pipeline.py:329] - [Pipeline] Query refined: Why is This faster?
2026-02-03 22:07:42 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #7 for session tracking_test_session
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 445
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:46 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:48 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:48 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:48 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #8 for session tracking_test_session
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 478
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:07:51 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:07:53 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:07:53 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:07:53 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:07:56 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:07:56 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:219] - Use pytorch device_name: cpu
2026-02-03 22:07:56 - sentence_transformers.SentenceTransformer - INFO - [SentenceTransformer.py:227] - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-02-03 22:07:57 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:57 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:07:57 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:57 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:07:57 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:57 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:58 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-03 22:07:59 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:59 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:59 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:07:59 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-03 22:07:59 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:08:00 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-03 22:08:00 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-03 22:08:00 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-03 22:08:00 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-03 22:08:00 - httpx - INFO - [_client.py:1013] - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-03 22:08:01 - httpx - INFO - [_client.py:1013] - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #1 for session summarization_test_session
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 15
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:01 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #2 for session summarization_test_session
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 125
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: True (heuristic only, confidence: 0.85)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: False (confidence: 0.0)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:229] - [Pipeline] Query not answerable - will return clarifying questions
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:03 - app.core.pipeline - INFO - [pipeline.py:339] - [Pipeline] Step 6: Generating clarifying questions (not answerable)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:355] - [Pipeline] Generated clarifying questions (LLM call made)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #3 for session summarization_test_session
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 228
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:06 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #4 for session summarization_test_session
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 285
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:08:10 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:11 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:11 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:11 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #5 for session summarization_test_session
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 340
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:15 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:16 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:16 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:16 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #6 for session summarization_test_session
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 381
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:08:20 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:21 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:21 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:21 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #7 for session summarization_test_session
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 436
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:25 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:26 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:26 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:26 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #8 for session summarization_test_session
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 474
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:30 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:32 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:32 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:32 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #9 for session summarization_test_session
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 513
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:35 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:36 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:36 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:36 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:126] - [Pipeline] Processing message #10 for session summarization_test_session
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:143] - [Pipeline] Token count: 567
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:185] - [Pipeline] === QUERY UNDERSTANDING PIPELINE ===
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:188] - [Pipeline] Step 1: Spelling check (rule-based)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:196] - [Pipeline] Step 2: Ambiguity check (rule-first, LLM fallback)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:212] - [Pipeline] Ambiguity: False (heuristic only, confidence: 0.95)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:217] - [Pipeline] Step 3: Answerability check (similarity-based, NO LLM)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:225] - [Pipeline] Answerable: True (confidence: 0.9)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:235] - [Pipeline] Step 4: Context retrieval (selective)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:243] - [Pipeline] Expanding context to 3 turns (pronoun/contrast detected)
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:250] - [Pipeline] Pronouns detected - including open_questions
2026-02-03 22:08:40 - app.core.pipeline - INFO - [pipeline.py:259] - [Pipeline] Memory fields used: []
2026-02-03 22:08:41 - app.core.pipeline - INFO - [pipeline.py:314] - [Pipeline] Generated concise final_augmented_context via LLM
2026-02-03 22:08:41 - app.core.pipeline - INFO - [pipeline.py:324] - [Pipeline] Step 5: Query refinement (rule-based entity replacement)
2026-02-03 22:08:41 - app.core.pipeline - INFO - [pipeline.py:361] - [Pipeline] Step 6: Generating LLM response (answerable)
2026-02-03 22:08:44 - app.core.pipeline - INFO - [pipeline.py:379] - [Pipeline] Generated LLM response
