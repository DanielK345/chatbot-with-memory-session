# Chat Bot with Memory

A production-ready conversational AI assistant with advanced memory management, session persistence, query understanding, and real-time deployment capability.

## Features

### Current Features âœ…

- **Conversation Memory**: Persistent session storage with automatic context retrieval
- **Session Summarization**: Token-aware summarization when conversations exceed threshold (default 10K tokens)
- **Query Understanding**: 
  - Ambiguous query detection
  - Query rewriting for clarity
  - Context augmentation
  - Clarifying question generation
- **Multi-LLM Support**: 
  - Google Gemini (primary)
  - Ollama (local/self-hosted fallback)
- **Conversation Logging**: Automatic JSON Lines logging with metadata (timestamps, token counts, ambiguity flags)
- **Rich UI**: Streamlit-based web interface with markdown rendering, styled chat bubbles, orange theme
- **Comprehensive Testing**: 6 test suites covering core functionality
- **CI/CD Pipeline**: GitHub Actions with automated testing and deployment hooks

### Deployment âœ…

- **Multi-Platform Ready**:
  - Railway (via Dockerfile + start.sh)
  - Render (FastAPI + Streamlit as separate services)
  - Heroku (via Procfile)
- **Production Server**: Gunicorn with Uvicorn workers
- **Environment Variables**: Port and concurrency auto-configuration
- **Docker Optimization**: Minimal images with build-time dependencies only

### Future Roadmap ğŸš€

1. **[CI/CD Pipeline Enhancement]**
   - GitHub Actions workflows for Docker image builds and pushes
   - Multi-platform deployment automation
   - Smoke tests and performance benchmarks
   - Branch protection and secrets scanning

2. **[User Authentication]**
   - JWT-based authentication
   - User registration/login endpoints
   - Role-based access control (RBAC)
   - Optional OAuth2 (Google, GitHub) and Auth0 integration
   - Per-user conversation isolation

3. **[PDF Upload & Document Storage]**
   - PDF upload endpoint with validation
   - Text extraction from PDFs
   - S3/MinIO storage integration
   - Similarity search in documents
   - Vector embeddings (optional: Pinecone)
   - Document-based RAG (Retrieval Augmented Generation)

4. **[Agentic Web Search]**
   - LangChain agent integration
   - DuckDuckGo/SerpAPI web search
   - Uncertainty-based search triggering
   - Factual query detection
   - Search result caching
   - Source attribution

## Query Understanding Pipeline

### ğŸ”„ New Workflow (v1.0+)

The query understanding pipeline processes every user query through 6 sequential steps to ensure clarity before LLM response generation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INCOMING USER QUERY                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  STEP 1: SPELLING CHECK               â”‚
         â”‚  âœ“ Rule-based (NO LLM)               â”‚
         â”‚  âœ“ Corrects typos & grammar          â”‚
         â”‚  âœ“ Fast, deterministic               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  STEP 2: AMBIGUITY DETECTION          â”‚
         â”‚  âœ“ Rule-first (heuristics)           â”‚
         â”‚  âœ“ 6 ambiguity rules:               â”‚
         â”‚    - RULE 1: Pronouns (it, they)    â”‚
         â”‚    - RULE 1b: Anaphoric (same, like)â”‚
         â”‚    - RULE 1c: Which-one patterns    â”‚
         â”‚    - RULE 2: Very short questions   â”‚
         â”‚    - RULE 2b: Choose without objectâ”‚
         â”‚    - RULE 3: Unclear intent         â”‚
         â”‚  âœ“ LLM fallback if uncertain        â”‚
         â”‚  âœ“ Logs confidence score            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                     â”‚
        CLEAR            AMBIGUOUS
           â”‚                     â”‚
           â†“                     â†“
      CONTINUE         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ RULE 3b: Is it still â”‚
                       â”‚ fixable with context?â”‚
                       â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                          â”‚              â”‚
                       YESâ”‚              â”‚NO
                          â†“              â†“
                      CONTINUE       UNCLEAR
                                         â”‚
                                         â†“
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ STEP 6b: Generate        â”‚
                      â”‚ Clarifying Questions     â”‚
                      â”‚ + Return instead of LLM  â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  STEP 3: ANSWERABILITY CHECK          â”‚
         â”‚  âœ“ Similarity-based (NO LLM)         â”‚
         â”‚  âœ“ Compares to known patterns       â”‚
         â”‚  âœ“ Checks if answerable by system   â”‚
         â”‚  âœ“ Falls back to clarifying Qs      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                     â”‚
       ANSWERABLE         NOT ANSWERABLE
           â”‚                     â”‚
           â†“                     â†“
       CONTINUE          (Clarifying Qs)
                                 â”‚
           â”‚
           â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  STEP 4: CONTEXT RETRIEVAL            â”‚
         â”‚  âœ“ Selective memory augmentation     â”‚
         â”‚  âœ“ Detects pronouns â†’ get history   â”‚
         â”‚  âœ“ Extracts key facts, decisions    â”‚
         â”‚  âœ“ Aggressive filtering (no bloat)  â”‚
         â”‚  âœ“ Max 3-turn lookback              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  STEP 5: QUERY REFINEMENT             â”‚
         â”‚  âœ“ Pronoun detection (regex)         â”‚
         â”‚  âœ“ Entity extraction from cache:     â”‚
         â”‚    - Last 3 queries (lightweight)    â”‚
         â”‚    - Capitalized word extraction    â”‚
         â”‚  âœ“ LLM rewriting (Qwen2.5-1.5B):    â”‚
         â”‚    - "Replace [pronouns] with       â”‚
         â”‚      [entities] in query"           â”‚
         â”‚    - Max 20 tokens response         â”‚
         â”‚    - 2-3x faster than llama3.1      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  STEP 6: LLM RESPONSE GENERATION      â”‚
         â”‚  âœ“ Build system + user prompt       â”‚
         â”‚  âœ“ Include augmented context        â”‚
         â”‚  âœ“ Generate response               â”‚
         â”‚  âœ“ Log everything                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        RESPONSE + METADATA            â”‚
         â”‚  âœ“ Answer text                       â”‚
         â”‚  âœ“ Query understanding results       â”‚
         â”‚  âœ“ Token usage statistics            â”‚
         â”‚  âœ“ Refinement details                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Principles

| Principle | Implementation |
|-----------|-----------------|
| **Prefer Early Exits** | Spelling corrected â†’ Fast success path |
| **Rule-First** | 6 heuristic rules before LLM for ambiguity |
| **Minimal LLM Usage** | Only use LLM when necessary (target <30%) |
| **Aggressive Filtering** | Context limited to recent messages + memory |
| **Lightweight Models** | Qwen2.5-1.5B for refinement (2-3x faster) |
| **Never Guess** | Clarifying questions instead of assumptions |
| **Fast Processing** | <500ms per query (without LLM delays) |

### Components

#### 1. **Spelling Checker** (`app/query_understanding/spelling_check.py`)
```python
# Corrects typos automatically
checker = SpellingChecker()
result = checker.check("whats the best libary for ML?")
# â†’ "what's the best library for ML?"
```

#### 2. **Ambiguity Detector** (`app/query_understanding/ambiguity.py`)
```python
# Detects 6 types of ambiguous queries
detector = AmbiguityDetector(llm_client)
analysis = await detector.detect("How does it compare?", messages)
# â†’ is_ambiguous=True, rule="RULE 1", reason="Pronoun 'it' without clear antecedent"
```

#### 3. **Query Refiner** (`app/query_understanding/query_refiner.py`)
```python
# Replaces pronouns with entities using lightweight LLM
refiner = QueryRefiner(llm_client)  # Uses Qwen2.5-1.5B by default
refined = await refiner.refine("How does it perform?")
# Cache: ["TensorFlow is fast", "PyTorch is flexible"]
# â†’ "How does TensorFlow perform?" or "How does PyTorch perform?"
```

**Query Refinement Details:**
- **Pronoun Detection**: it, they, them, this, that, he, she
- **Entity Extraction**: From last 3 queries (lightweight cache)
- **LLM Rewriting**: Qwen2.5-1.5B model (1.5B params vs 8B for llama3.1)
- **Performance**: 2-3x faster than standard models
- **Fallback**: Auto-fallback to active LLM if Qwen unavailable

#### 4. **Context Augmenter** (`app/query_understanding/context.py`)
```python
# Intelligently retrieves session memory
augmenter = ContextAugmenter()
context, fields = augmenter.augment(
    query="How does it compare?",
    messages=messages,
    session_memory=memory,
    needed_fields=["key_facts", "decisions"]
)
```

#### 5. **Clarifying Question Generator** (`app/query_understanding/clarifier.py`)
```python
# Generates clarifying questions if query still unclear
clarifier = ClarifyingQuestionGenerator(llm_client)
questions = await clarifier.generate(
    "What should I choose?",  # Missing object
    messages=messages
)
# â†’ ["Choose what? (library, algorithm, etc.)",
#    "What's your main priority? (speed, accuracy, etc.)",
#    "What's your use case?"]
```

---

## Logging System

The system generates **three types of detailed logs** for analysis:

### ğŸ“Š Log Types

| Log Type | File | Contents | Use Case |
|----------|------|----------|----------|
| **Conversation** | `conversations_*.log` | User-assistant pairs + metadata | Analyze conversations, user behavior |
| **User Query** | `user_queries_*.log` | Original â†’ refined query + context | Debug ambiguity detection, refinement |
| **Session Summary** | `session_summaries_*.log` | Session facts, decisions, summary | Understand session evolution |

### ğŸ” Log Structure

#### **Conversation Log** (`conversations_*.log`)
```json
{
  "timestamp": "2026-02-04T11:38:06.123456",
  "session_id": "session-123",
  "user": "How does it perform?",
  "assistant": "TensorFlow performs well for...",
  "metadata": {
    "is_answerable": true,
    "token_count": 1234,
    "summarization_triggered": false,
    "pipeline_metadata": {
      "spelling_check_used": false,
      "ambiguity_llm_used": true,
      "answerability_check_passed": true,
      "context_expanded": true,
      "refinement_applied": true,
      "llm_call_made": true
    },
    "llm_usage_percentage": "45.2%"
  }
}
```

#### **User Query Log** (`user_queries_*.log`)
```json
{
  "timestamp": "2026-02-04T11:38:06.123456",
  "session_id": "session-123",
  "original_query": "How does it perform?",
  "is_ambiguous": true,
  "rewritten_query": "How does TensorFlow perform?",
  "needed_context_from_memory": [
    "user_profile.prefs: [wants speed, flexibility]",
    "key_facts: [using TensorFlow in project]",
    "decisions: [chose TensorFlow over PyTorch]"
  ],
  "clarifying_questions": [],
  "final_augmented_context": "Recent discussion: TensorFlow chosen for project..."
}
```

#### **Session Summary Log** (`session_summaries_*.log`)
```json
{
  "timestamp": "2026-02-04T11:38:06.123456",
  "session_id": "session-123",
  "session_summary": {
    "user_profile": {
      "prefs": ["speed", "flexibility"],
      "constraints": ["budget: limited", "team: 2 engineers"]
    },
    "key_facts": [
      "Building ML system for production",
      "Team has PyTorch experience"
    ],
    "decisions": [
      "Chose TensorFlow for deployment",
      "Using transfer learning approach"
    ],
    "open_questions": [
      "How to optimize training speed?"
    ]
  },
  "message_range_summarized": {
    "from": 0,
    "to": 42
  }
}
```

---

## Running Tests & Generating Logs

### ğŸ“ Test Scripts Location
```
tests/
â”œâ”€â”€ test_ambiguous_query_detection.py    â† Ambiguity detection + all 6 rules
â”œâ”€â”€ test_query_refinement.py              â† Query refinement with LLM
â”œâ”€â”€ test_session_summarization.py         â† Session memory & summarization
â”œâ”€â”€ test_conversation_logging.py          â† Conversation persistence
â”œâ”€â”€ test_cli_demo.py                      â† CLI interface testing
â”œâ”€â”€ test_streamlit_app.py                 â† Streamlit UI testing
â””â”€â”€ run_tests.py                          â† Run all tests
```

### ğŸš€ Running Tests & Generating Logs

#### **1. Test Ambiguity Detection (All 6 Rules)**
```bash
# Run: Tests all 6 ambiguity rules with natural conversation
python tests/test_ambiguous_query_detection.py

# Generates logs:
# logs/ambiguous_query_detection/
# â”œâ”€â”€ conversations_test.log      (user-assistant pairs with metadata)
# â”œâ”€â”€ user_queries_test.log        (original + rewritten queries, ambiguity flags)
# â””â”€â”€ session_summaries_test.log   (session memory evolution)

# Example output:
# [âœ“] Test: Ambiguous query detection
# [âœ“] Query 1: "We're building a machine learning system" â†’ CLEAR (100% confidence)
# [âœ“] Query 3: "How does it perform?" â†’ AMBIGUOUS (RULE 1: pronoun without antecedent)
# [âœ“] Query 7: "Which one do you prefer?" â†’ AMBIGUOUS (RULE 1c: which-one without context)
# [âœ“] Query 23: "It?" â†’ AMBIGUOUS (RULE 2: very short question)
# [âœ“] Overall: 24/28 queries correctly classified (85.7% accuracy)
```

#### **2. Test Query Refinement (Lightweight Model)**
```bash
# Run: Tests pronoun replacement with Qwen2.5-1.5B model
python tests/test_query_refinement.py

# Prerequisites: Ollama running with qwen2.5:1.5b pulled
# ollama pull qwen2.5:1.5b
# ollama serve

# Generates logs:
# logs/query_refinement/
# â”œâ”€â”€ conversations_test.log
# â”œâ”€â”€ user_queries_test.log        (shows rewritten_query field populated)
# â””â”€â”€ session_summaries_test.log

# Example output:
# [Original] "How does it perform?"
# [Refined]  "How does TensorFlow perform?" âœ“
# [Cache]    ["TensorFlow is fast", "PyTorch is flexible"]
```

#### **3. Test Session Summarization**
```bash
# Run: Tests token-aware summarization when context exceeds threshold
python tests/test_session_summarization.py

# Generates logs:
# logs/session_summarization/
# â”œâ”€â”€ conversations_test.log
# â”œâ”€â”€ user_queries_test.log
# â””â”€â”€ session_summaries_test.log  (shows summarization_triggered + summary content)

# Example output:
# Token count: 2500
# [âœ“] Summarization triggered at 10000 tokens
# [âœ“] Extracted 5 key facts
# [âœ“] Tracked 3 decisions
# [âœ“] Found 2 open questions
```

#### **4. Run All Tests**
```bash
# Run entire test suite
python tests/run_tests.py

# Generates all logs across test directories:
logs/
â”œâ”€â”€ ambiguous_query_detection/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log
â”œâ”€â”€ query_refinement/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log
â”œâ”€â”€ session_summarization/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log
â””â”€â”€ ...
```

### ğŸ“‚ Analyzing Generated Logs

#### **View Conversation Logs**
```bash
# Pretty-print conversation log
python -c "
import json
with open('logs/ambiguous_query_detection/conversations_test.log') as f:
    for line in f:
        if line.strip():
            data = json.loads(line)
            print(f\"User: {data['user']}\")
            print(f\"Assistant: {data['assistant'][:100]}...\")
            print(f\"Ambiguity: {data['metadata']['pipeline_metadata']['ambiguity_llm_used']}\")
            print()
"
```

#### **View User Query Logs** 
```bash
# Show original â†’ refined queries
python -c "
import json
with open('logs/ambiguous_query_detection/user_queries_test.log') as f:
    for line in f:
        if line.strip():
            data = json.loads(line)
            orig = data['original_query']
            refined = data['rewritten_query']
            if refined and refined != orig:
                print(f\"Original: {orig}\")
                print(f\"Refined:  {refined}\")
                print()
"
```

#### **View Session Summaries**
```bash
# Show session evolution
python -c "
import json
with open('logs/ambiguous_query_detection/session_summaries_test.log') as f:
    for line in f:
        if line.strip():
            data = json.loads(line)
            summary = data['session_summary']
            print(f\"Facts: {summary.get('key_facts', [])}\")
            print(f\"Decisions: {summary.get('decisions', [])}\")
            print(f\"Open Qs: {summary.get('open_questions', [])}\")
            print()
"
```

### ğŸ”§ Log Directory Structure

```
logs/
â”œâ”€â”€ ambiguous_query_detection/
â”‚   â”œâ”€â”€ conversations_test.log       â† User-assistant conversations
â”‚   â”œâ”€â”€ user_queries_test.log        â† Query analysis (ambiguity, refinement)
â”‚   â””â”€â”€ session_summaries_test.log   â† Memory & summaries
â”œâ”€â”€ query_refinement/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log        â† Shows rewritten_query field
â”‚   â””â”€â”€ session_summaries_test.log
â”œâ”€â”€ session_summarization/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log   â† Shows summarization triggers
â”œâ”€â”€ app.log                          â† Application debug logs
â””â”€â”€ [others]/
```

### ğŸ“Š Log Insights

**From Conversation Logs:**
- Track LLM call frequency (target: <30%)
- Monitor token usage per query
- Analyze response quality

**From User Query Logs:**
- See which queries are being refined
- Validate ambiguity detection accuracy
- Track context augmentation effectiveness

**From Session Summary Logs:**
- Monitor session memory evolution
- Identify when summarization triggers
- Validate fact extraction quality



## Quick Start

### Prerequisites
- Python 3.10+
- Docker (for containerized deployment)
- API Keys: Google Generative AI (Gemini)

### Installation

1. **Clone and Setup**
```bash
git clone <repo-url>
cd chat-bot-with-memory
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. **Configure Environment**
```bash
cp .env.example .env
# Edit .env with your API keys:
# - GOOGLE_API_KEY
# - OLLAMA_HOST (optional, for fallback)
```

3. **Run Locally**

**Option A: CLI Demo**
```bash
python cli_demo.py
```

**Option B: Streamlit UI**
```bash
streamlit run streamlit_app.py
```

**Option C: FastAPI Backend**
```bash
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Docker Deployment (Local)

**API Service**
```bash
docker build -t chat-bot-api .
docker run -e PORT=8000 -e GOOGLE_API_KEY=your-key -p 8000:8000 chat-bot-api
```

**Streamlit UI**
```bash
docker build -f Dockerfile.streamlit -t chat-bot-ui .
docker run -e PORT=8501 -p 8501:8501 chat-bot-ui
```

## Deployment Guides

### Railway
1. Push repo to GitHub
2. Create new project on https://railway.app
3. Connect GitHub repo
4. Choose Docker deployment
5. Set environment variables (API keys)
6. Deploy â€” Railway auto-builds and runs start.sh

### Render
1. Create two Web Services:
   - **Service A (API)**: 
     - Connect repo, choose Docker
     - Set Dockerfile Path: `Dockerfile`
     - Set Start Command: `/app/start.sh`
   - **Service B (UI)**: 
     - Connect repo, choose Docker
     - Set Dockerfile Path: `Dockerfile.streamlit`
2. Set environment variables on each service
3. Deploy

### Heroku
```bash
# Install Heroku CLI
brew tap heroku/brew && brew install heroku-cli
# or on Windows: choco install heroku-cli

# Login and create app
heroku login
heroku create your-app-name

# Set environment variables
heroku config:set GOOGLE_API_KEY=your-key

# Push to Heroku
git push heroku main
```

## File Structure

```
chat-bot-with-memory/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ api/                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ docs.py          # Document upload (future)
â”‚   â”‚   â””â”€â”€ auth.py          # Authentication (future)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”‚   â””â”€â”€ token_counter.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py        # Base LLM client
â”‚   â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”‚   â””â”€â”€ json_guard.py    # JSON parsing utilities
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ session_store.py # Session persistence
â”‚   â”‚   â”œâ”€â”€ schemas.py       # Data models
â”‚   â”‚   â”œâ”€â”€ summarizer.py    # Token-aware summarization
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ query_understanding/
â”‚   â”‚   â”œâ”€â”€ ambiguity.py     # Detect ambiguous queries
â”‚   â”‚   â”œâ”€â”€ clarifier.py     # Generate clarifying questions
â”‚   â”‚   â”œâ”€â”€ context.py       # Context augmentation
â”‚   â”‚   â”œâ”€â”€ rewrite.py       # Query rewriting
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py       # ConversationLogger
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_session_summarization.py
â”‚   â”œâ”€â”€ test_ambiguous_query_detection.py
â”‚   â”œâ”€â”€ test_query_refinement.py
â”‚   â”œâ”€â”€ test_conversation_logging.py
â”‚   â”œâ”€â”€ test_cli_demo.py
â”‚   â”œâ”€â”€ test_streamlit_app.py
â”‚   â””â”€â”€ run_tests.py
â”œâ”€â”€ docs/                    # Implementation guides
â”‚   â”œâ”€â”€ 01_CI_CD_PIPELINE.md
â”‚   â”œâ”€â”€ 02_AUTHENTICATION.md
â”‚   â”œâ”€â”€ 03_MONITORING.md
â”‚   â”œâ”€â”€ 04_PDF_UPLOAD_S3.md
â”‚   â”œâ”€â”€ 05_AGENTIC_WEB_SEARCH.md
â”‚   â””â”€â”€ CONVERSATION_LOGGING.md
â”œâ”€â”€ data/                    # Persistent data
â”‚   â”œâ”€â”€ sessions/            # Session summaries (JSON)
â”‚   â””â”€â”€ conversations/       # Conversation logs (JSONL)
â”œâ”€â”€ logs/                    # Application logs
â”œâ”€â”€ cli_demo.py              # CLI interface
â”œâ”€â”€ streamlit_app.py         # Streamlit web UI
â”œâ”€â”€ Dockerfile               # API service container
â”œâ”€â”€ Dockerfile.streamlit     # Streamlit UI container
â”œâ”€â”€ start.sh                 # Production entrypoint (Gunicorn)
â”œâ”€â”€ start_streamlit.sh       # Streamlit entrypoint
â”œâ”€â”€ Procfile                 # Heroku process definition
â”œâ”€â”€ runtime.txt              # Python runtime version (Heroku)
â”œâ”€â”€ docker-compose.yml       # Development orchestration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .dockerignore            # Docker build optimization
â”œâ”€â”€ .env.example             # Environment template
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ tests.yml            # GitHub Actions CI/CD
â””â”€â”€ README.md                # This file
```

## Environment Variables

```bash
# LLM Configuration
GOOGLE_API_KEY=your-gemini-api-key
OLLAMA_HOST=http://localhost:11434  # For fallback

# Session Management
SESSION_TOKEN_THRESHOLD=10000        # Summarize at this token count
SESSION_STORAGE_TYPE=file            # or 'redis'
REDIS_URL=redis://localhost:6379     # If using Redis

# Logging
LOG_LEVEL=INFO
LOG_DIR=./logs

# API
PORT=8000
WEB_CONCURRENCY=1                    # Gunicorn workers

# Future Features
ENABLE_AUTHENTICATION=false          # Enable auth (docs/02)
ENABLE_WEB_SEARCH=false              # Enable web search (docs/05)
ENABLE_DOCUMENT_UPLOAD=false         # Enable PDF upload (docs/04)

# Monitoring (docs/03)
ENABLE_PROMETHEUS=false
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
```

## API Endpoints

### Chat
```
POST /chat
Body: {
  "message": "Your question",
  "session_id": "session-123"
}

Response: {
  "response": "Assistant's answer",
  "session_id": "session-123",
  "token_count": 1234,
  "context_used": true
}
```

## Testing

Run full test suite:
```bash
python tests/run_tests.py
```

Run specific test:
```bash
pytest tests/test_session_summarization.py -v
```

Test with coverage:
```bash
pytest tests/ --cov=app --cov-report=html
```

**Session Management**
```bash
# View session info
python session_manager.py -i default_session

# Delete session
python session_manager.py -d default_session

# Delete all sessions
python session_manager.py -d
```

## Security Checklist

- [ ] Store API keys securely (environment variables, not in code)
- [ ] Use HTTPS in production (Railway/Render provide this)
- [ ] Implement rate limiting on API endpoints (future)
- [ ] Validate and sanitize all user inputs
- [ ] Log access to sensitive operations
- [ ] Regularly update dependencies: `pip list --outdated`
- [ ] Enable authentication before production deployment
- [ ] Configure CORS appropriately
- [ ] Use private S3/MinIO buckets for documents

### Port Already in Use
```bash
# Find process using port 8000
lsof -i :8000
kill -9 <PID>
```

### Gemini API Errors
- Verify `GOOGLE_API_KEY` is set correctly
- Check API quota on Google Cloud Console
- Ensure billing is enabled

### Streamlit Not Loading
- Verify `streamlit_app.py` exists
- Check port 8501 is accessible
- Review Streamlit logs: `streamlit run streamlit_app.py --logger.level=debug`

### Docker Build Failures
- Clear cache: `docker system prune -a`
- Check `requirements.txt` for invalid packages
- Verify `start.sh` has execute permissions

## Contributing

1. Create feature branch: `git checkout -b feature/my-feature`
2. Make changes and test: `python tests/run_tests.py`
3. Commit: `git commit -am "Add my feature"`
4. Push: `git push origin feature/my-feature`
5. Open PR on GitHub

## Implementation Roadmap Priority

1. **High Priority** (Next Sprint):
   - Authentication (enable multi-user support)
   - CI/CD automation (faster deployments)

2. **Medium Priority** (Following Sprint):
   - PDF upload & document storage
   - Web search for current information

## License

MIT

## Support

For issues, questions, or feature requests, open a GitHub issue.

---

**Last Updated**: February 2026
**Current Version**: 1.0.0
**Status**: Production Ready (Core Features)
