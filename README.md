# Chat Bot with Memory

A production-ready conversational AI assistant with advanced memory management, session persistence, query understanding, and real-time deployment capability.

## Features

### Current Features âœ…

- **Conversation Memory**: Persistent session storage with automatic context retrieval
- **Session Summarization**: Token-aware summarization when conversations exceed threshold (default 10K tokens)
- **Query Understanding**: 
  - Ambiguous query detection
  - Query rewriting for clarity
  - Context augmentation
  - Clarifying question generation
- **Multi-LLM Support**: 
  - Google Gemini (primary)
  - Ollama (local/self-hosted fallback)
- **Conversation Logging**: Automatic JSON Lines logging with metadata (timestamps, token counts, ambiguity flags)
- **Rich UI**: Streamlit-based web interface with markdown rendering, styled chat bubbles, orange theme
- **Comprehensive Testing**: 6 test suites covering core functionality
- **CI/CD Pipeline**: GitHub Actions with automated testing and deployment hooks

### Deployment âœ…

- **Multi-Platform Ready**:
  - Railway (via Dockerfile + start.sh)
  - Render (FastAPI + Streamlit as separate services)
  - Heroku (via Procfile)
- **Production Server**: Gunicorn with Uvicorn workers
- **Environment Variables**: Port and concurrency auto-configuration
- **Docker Optimization**: Minimal images with build-time dependencies only

### Future Roadmap ğŸš€

1. **[CI/CD Pipeline Enhancement]**
   - GitHub Actions workflows for Docker image builds and pushes
   - Multi-platform deployment automation
   - Smoke tests and performance benchmarks
   - Branch protection and secrets scanning

2. **[User Authentication]**
   - JWT-based authentication
   - User registration/login endpoints
   - Role-based access control (RBAC)
   - Optional OAuth2 (Google, GitHub) and Auth0 integration
   - Per-user conversation isolation

3. **[PDF Upload & Document Storage]**
   - PDF upload endpoint with validation
   - Text extraction from PDFs
   - S3/MinIO storage integration
   - Similarity search in documents
   - Vector embeddings (optional: Pinecone)
   - Document-based RAG (Retrieval Augmented Generation)

4. **[Agentic Web Search]**
   - LangChain agent integration
   - DuckDuckGo/SerpAPI web search
   - Uncertainty-based search triggering
   - Factual query detection
   - Search result caching
   - Source attribution

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI      â”‚ â† Rich markdown, styled bubbles
â”‚  (Optional Render)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ HTTP/JSON
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Backend    â”‚ â† Gunicorn + Uvicorn
â”‚  (Railway/Render)    â”‚
â”‚                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  Chat Pipeline   â”‚ â”‚ â† Orchestrator
â”‚ â”‚  - Query Understandâ”‚
â”‚ â”‚  - Memory Manage â”‚
â”‚ â”‚  - LLM Call      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ LLM Clients      â”‚ â”‚ â† Gemini, Ollama
â”‚ â”‚ Token Counter    â”‚ â”‚
â”‚ â”‚ Summarizer       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                     â†“            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Session Storeâ”‚  â”‚LLM APIs        â”‚  â”‚Logging       â”‚
â”‚ (JSON Files)â”‚  â”‚(Gemini/Ollama) â”‚  â”‚ (JSON Lines) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites
- Python 3.10+
- Docker (for containerized deployment)
- API Keys: Google Generative AI (Gemini)

### Installation

1. **Clone and Setup**
```bash
git clone <repo-url>
cd chat-bot-with-memory
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. **Configure Environment**
```bash
cp .env.example .env
# Edit .env with your API keys:
# - GOOGLE_API_KEY
# - OLLAMA_HOST (optional, for fallback)
```

3. **Run Locally**

**Option A: CLI Demo**
```bash
python cli_demo.py
```

**Option B: Streamlit UI**
```bash
streamlit run streamlit_app.py
```

**Option C: FastAPI Backend**
```bash
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Docker Deployment (Local)

**API Service**
```bash
docker build -t chat-bot-api .
docker run -e PORT=8000 -e GOOGLE_API_KEY=your-key -p 8000:8000 chat-bot-api
```

**Streamlit UI**
```bash
docker build -f Dockerfile.streamlit -t chat-bot-ui .
docker run -e PORT=8501 -p 8501:8501 chat-bot-ui
```

## Deployment Guides

### Railway
1. Push repo to GitHub
2. Create new project on https://railway.app
3. Connect GitHub repo
4. Choose Docker deployment
5. Set environment variables (API keys)
6. Deploy â€” Railway auto-builds and runs start.sh

### Render
1. Create two Web Services:
   - **Service A (API)**: 
     - Connect repo, choose Docker
     - Set Dockerfile Path: `Dockerfile`
     - Set Start Command: `/app/start.sh`
   - **Service B (UI)**: 
     - Connect repo, choose Docker
     - Set Dockerfile Path: `Dockerfile.streamlit`
2. Set environment variables on each service
3. Deploy

### Heroku
```bash
# Install Heroku CLI
brew tap heroku/brew && brew install heroku-cli
# or on Windows: choco install heroku-cli

# Login and create app
heroku login
heroku create your-app-name

# Set environment variables
heroku config:set GOOGLE_API_KEY=your-key

# Push to Heroku
git push heroku main
```

## File Structure

```
chat-bot-with-memory/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ api/                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ docs.py          # Document upload (future)
â”‚   â”‚   â””â”€â”€ auth.py          # Authentication (future)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”‚   â””â”€â”€ token_counter.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py        # Base LLM client
â”‚   â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”‚   â””â”€â”€ json_guard.py    # JSON parsing utilities
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ session_store.py # Session persistence
â”‚   â”‚   â”œâ”€â”€ schemas.py       # Data models
â”‚   â”‚   â”œâ”€â”€ summarizer.py    # Token-aware summarization
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ query_understanding/
â”‚   â”‚   â”œâ”€â”€ ambiguity.py     # Detect ambiguous queries
â”‚   â”‚   â”œâ”€â”€ clarifier.py     # Generate clarifying questions
â”‚   â”‚   â”œâ”€â”€ context.py       # Context augmentation
â”‚   â”‚   â”œâ”€â”€ rewrite.py       # Query rewriting
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py       # ConversationLogger
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_session_summarization.py
â”‚   â”œâ”€â”€ test_ambiguous_query_detection.py
â”‚   â”œâ”€â”€ test_query_refinement.py
â”‚   â”œâ”€â”€ test_conversation_logging.py
â”‚   â”œâ”€â”€ test_cli_demo.py
â”‚   â”œâ”€â”€ test_streamlit_app.py
â”‚   â””â”€â”€ run_tests.py
â”œâ”€â”€ docs/                    # Implementation guides
â”‚   â”œâ”€â”€ 01_CI_CD_PIPELINE.md
â”‚   â”œâ”€â”€ 02_AUTHENTICATION.md
â”‚   â”œâ”€â”€ 03_MONITORING.md
â”‚   â”œâ”€â”€ 04_PDF_UPLOAD_S3.md
â”‚   â”œâ”€â”€ 05_AGENTIC_WEB_SEARCH.md
â”‚   â””â”€â”€ CONVERSATION_LOGGING.md
â”œâ”€â”€ data/                    # Persistent data
â”‚   â”œâ”€â”€ sessions/            # Session summaries (JSON)
â”‚   â””â”€â”€ conversations/       # Conversation logs (JSONL)
â”œâ”€â”€ logs/                    # Application logs
â”œâ”€â”€ cli_demo.py              # CLI interface
â”œâ”€â”€ streamlit_app.py         # Streamlit web UI
â”œâ”€â”€ Dockerfile               # API service container
â”œâ”€â”€ Dockerfile.streamlit     # Streamlit UI container
â”œâ”€â”€ start.sh                 # Production entrypoint (Gunicorn)
â”œâ”€â”€ start_streamlit.sh       # Streamlit entrypoint
â”œâ”€â”€ Procfile                 # Heroku process definition
â”œâ”€â”€ runtime.txt              # Python runtime version (Heroku)
â”œâ”€â”€ docker-compose.yml       # Development orchestration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .dockerignore            # Docker build optimization
â”œâ”€â”€ .env.example             # Environment template
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ tests.yml            # GitHub Actions CI/CD
â””â”€â”€ README.md                # This file
```

## Environment Variables

```bash
# LLM Configuration
GOOGLE_API_KEY=your-gemini-api-key
OLLAMA_HOST=http://localhost:11434  # For fallback

# Session Management
SESSION_TOKEN_THRESHOLD=10000        # Summarize at this token count
SESSION_STORAGE_TYPE=file            # or 'redis'
REDIS_URL=redis://localhost:6379     # If using Redis

# Logging
LOG_LEVEL=INFO
LOG_DIR=./logs

# API
PORT=8000
WEB_CONCURRENCY=1                    # Gunicorn workers

# Future Features
ENABLE_AUTHENTICATION=false          # Enable auth (docs/02)
ENABLE_WEB_SEARCH=false              # Enable web search (docs/05)
ENABLE_DOCUMENT_UPLOAD=false         # Enable PDF upload (docs/04)

# Monitoring (docs/03)
ENABLE_PROMETHEUS=false
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
```

## API Endpoints

### Chat
```
POST /chat
Body: {
  "message": "Your question",
  "session_id": "session-123"
}

Response: {
  "response": "Assistant's answer",
  "session_id": "session-123",
  "token_count": 1234,
  "context_used": true
}
```

## Testing

Run full test suite:
```bash
python tests/run_tests.py
```

Run specific test:
```bash
pytest tests/test_session_summarization.py -v
```

Test with coverage:
```bash
pytest tests/ --cov=app --cov-report=html
```

**Session Management**
```bash
# View session info
python session_manager.py -i default_session

# Delete session
python session_manager.py -d default_session

# Delete all sessions
python session_manager.py -d
```

## Security Checklist

- [ ] Store API keys securely (environment variables, not in code)
- [ ] Use HTTPS in production (Railway/Render provide this)
- [ ] Implement rate limiting on API endpoints (future)
- [ ] Validate and sanitize all user inputs
- [ ] Log access to sensitive operations
- [ ] Regularly update dependencies: `pip list --outdated`
- [ ] Enable authentication before production deployment
- [ ] Configure CORS appropriately
- [ ] Use private S3/MinIO buckets for documents

### Port Already in Use
```bash
# Find process using port 8000
lsof -i :8000
kill -9 <PID>
```

### Gemini API Errors
- Verify `GOOGLE_API_KEY` is set correctly
- Check API quota on Google Cloud Console
- Ensure billing is enabled

### Streamlit Not Loading
- Verify `streamlit_app.py` exists
- Check port 8501 is accessible
- Review Streamlit logs: `streamlit run streamlit_app.py --logger.level=debug`

### Docker Build Failures
- Clear cache: `docker system prune -a`
- Check `requirements.txt` for invalid packages
- Verify `start.sh` has execute permissions

## Contributing

1. Create feature branch: `git checkout -b feature/my-feature`
2. Make changes and test: `python tests/run_tests.py`
3. Commit: `git commit -am "Add my feature"`
4. Push: `git push origin feature/my-feature`
5. Open PR on GitHub

## Implementation Roadmap Priority

1. **High Priority** (Next Sprint):
   - Authentication (enable multi-user support)
   - CI/CD automation (faster deployments)

2. **Medium Priority** (Following Sprint):
   - PDF upload & document storage
   - Web search for current information

## License

MIT

## Support

For issues, questions, or feature requests, open a GitHub issue.

---

**Last Updated**: February 2026
**Current Version**: 1.0.0
**Status**: Production Ready (Core Features)
