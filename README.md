# Chat Bot with Memory

A production-ready conversational AI assistant with advanced memory management, session persistence, query understanding, and real-time deployment capability.

## App Interface

![Chat Bot UI](assets/UI.png)

*Streamlit-based web interface with rich markdown support, styled chat bubbles, and orange theme for an intuitive conversational experience.*

## Features

### Current Features âœ…

- **Conversation Memory**: Persistent session storage with automatic context retrieval
- **Session Summarization**: Token-aware summarization when conversations exceed threshold (default 10K tokens)
- **Query Understanding**: 
  - Ambiguous query detection
  - Query rewriting for clarity
  - Context augmentation
  - Clarifying question generation
- **Multi-LLM Support**: 
  - Google Gemini (primary)
  - Ollama (local/self-hosted fallback)
- **Conversation Logging**: Automatic JSON Lines logging with metadata (timestamps, token counts, ambiguity flags)
- **Rich UI**: Streamlit-based web interface with markdown rendering, styled chat bubbles, orange theme
- **Comprehensive Testing**: 6 test suites covering core functionality

## Query Understanding Pipeline

### ğŸ”„ New Workflow (v1.0+)

The query understanding pipeline processes every user query through 6 sequential steps to ensure clarity before LLM response generation:

```mermaid
graph TD
    A["ğŸ”¤ USER QUERY"]
    B["STEP 1: SPELLING CHECK<br/>Rule-based | No LLM<br/>Correct typos & grammar"]
    C["STEP 2: AMBIGUITY DETECTION<br/>ğŸ¤– Gemini fallback<br/>6 heuristic rules first"]
    D["CONTINUE âœ“"]
    E["Fixable with context?"]
    F["STEP 6b: CLARIFYING QUESTIONS<br/>ğŸ¤– Gemini<br/>Ask user for clarity"]
    G["STEP 3: ANSWERABILITY CHECK<br/>ğŸ¤– MiniLM<br/>Similarity matching"]
    H["CONTINUE âœ“"]
    I["CLARIFYING QUESTIONS"]
    J["STEP 4: CONTEXT RETRIEVAL<br/>Rule-based | No LLM<br/>Memory augmentation"]
    K["STEP 5: QUERY REFINEMENT<br/>ğŸ¤– Qwen 2.5-1.5B<br/>Pronoun â†’ Entity"]
    L["STEP 6: LLM RESPONSE GENERATION<br/>ğŸ¤– Gemini<br/>Generate + log"]
    M["âœ… RESPONSE + METADATA<br/>Answer | Tokens | Refinement"]

    A --> B --> C
    C -->|CLEAR| D
    C -->|AMBIGUOUS| E
    E -->|YES| D
    E -->|NO| F
    D --> G
    G -->|ANSWERABLE| H
    G -->|NOT ANSWERABLE| I
    H --> J --> K --> L --> M
    F --> M
    I --> M

    class A input
    class B,J stepNeutral
    class C,L stepOrange
    class D,H success
    class E decision
    class F,I danger
    class G,K stepPurple
    class M success

    classDef input fill:#e3f2fd,stroke:#0d47a1,stroke-width:3px,color:#0d47a1,font-weight:bold
    classDef stepNeutral fill:#eeeeee,stroke:#424242,stroke-width:3px,color:#212121,font-weight:bold
    classDef stepOrange fill:#ffe0b2,stroke:#e65100,stroke-width:3px,color:#bf360c,font-weight:bold
    classDef stepPurple fill:#e1bee7,stroke:#6a1b9a,stroke-width:3px,color:#4a148c,font-weight:bold
    classDef success fill:#c8e6c9,stroke:#1b5e20,stroke-width:3px,color:#1b5e20,font-weight:bold
    classDef danger fill:#ffccbc,stroke:#bf360c,stroke-width:3px,color:#bf360c,font-weight:bold
    classDef decision fill:#fff3e0,stroke:#ef6c00,stroke-width:3px,color:#e65100,font-weight:bold
  ```

### Components

#### 1. **Spelling Checker** (`app/query_understanding/spelling_check.py`)
```python
# Corrects typos automatically
checker = SpellingChecker()
result = checker.check("whats the best libary for ML?")
# â†’ "what's the best library for ML?"
```

#### 2. **Ambiguity Detector** (`app/query_understanding/ambiguity.py`)
```python
# Detects 6 types of ambiguous queries
detector = AmbiguityDetector(llm_client)
analysis = await detector.detect("How does it compare?", messages)
# â†’ is_ambiguous=True, rule="RULE 1", reason="Pronoun 'it' without clear antecedent"
```

#### 3. **Query Refiner** (`app/query_understanding/query_refiner.py`)
```python
# Replaces pronouns with entities using lightweight LLM
refiner = QueryRefiner(llm_client)  # Uses Qwen2.5-1.5B by default
refined = await refiner.refine("How does it perform?")
# Cache: ["TensorFlow is fast", "PyTorch is flexible"]
# â†’ "How does TensorFlow perform?" or "How does PyTorch perform?"
```

**Query Refinement Details:**
- **Pronoun Detection**: it, they, them, this, that, he, she
- **Entity Extraction**: From last 3 queries (lightweight cache)
- **LLM Rewriting**: Qwen2.5-1.5B model (1.5B params vs 8B for llama3.1)
- **Performance**: 2-3x faster than standard models
- **Fallback**: Auto-fallback to active LLM if Qwen unavailable

#### 4. **Context Augmenter** (`app/query_understanding/context.py`)
```python
# Intelligently retrieves session memory
augmenter = ContextAugmenter()
context, fields = augmenter.augment(
    query="How does it compare?",
    messages=messages,
    session_memory=memory,
    needed_fields=["key_facts", "decisions"]
)
```

#### 5. **Clarifying Question Generator** (`app/query_understanding/clarifier.py`)
```python
# Generates clarifying questions if query still unclear
clarifier = ClarifyingQuestionGenerator(llm_client)
questions = await clarifier.generate(
    "What should I choose?",  # Missing object
    messages=messages
)
# â†’ ["Choose what? (library, algorithm, etc.)",
#    "What's your main priority? (speed, accuracy, etc.)",
#    "What's your use case?"]
```

---

## Logging System

The system generates **three types of detailed logs** for analysis:

### ğŸ“Š Log Types

| Log Type | File | Contents | Use Case |
|----------|------|----------|----------|
| **Conversation** | `conversations_*.log` | User-assistant pairs + metadata | Analyze conversations, user behavior |
| **User Query** | `user_queries_*.log` | Original â†’ refined query + context | Debug ambiguity detection, refinement |
| **Session Summary** | `session_summaries_*.log` | Session facts, decisions, summary | Understand session evolution |

### ğŸ” Log Structure

#### **Conversation Log** (`conversations_*.log`)
```json
{
  "timestamp": "2026-02-04T11:38:06.123456",
  "session_id": "session-123",
  "user": "How does it perform?",
  "assistant": "TensorFlow performs well for...",
  "metadata": {
    "is_answerable": true,
    "token_count": 1234,
    "summarization_triggered": false,
    "pipeline_metadata": {
      "spelling_check_used": false,
      "ambiguity_llm_used": true,
      "answerability_check_passed": true,
      "context_expanded": true,
      "refinement_applied": true,
      "llm_call_made": true
    },
    "llm_usage_percentage": "45.2%"
  }
}
```

#### **User Query Log** (`user_queries_*.log`)
```json
{
  "timestamp": "2026-02-04T11:38:06.123456",
  "session_id": "session-123",
  "original_query": "How does it perform?",
  "is_ambiguous": true,
  "rewritten_query": "How does TensorFlow perform?",
  "needed_context_from_memory": [
    "user_profile.prefs: [wants speed, flexibility]",
    "key_facts: [using TensorFlow in project]",
    "decisions: [chose TensorFlow over PyTorch]"
  ],
  "clarifying_questions": [],
  "final_augmented_context": "Recent discussion: TensorFlow chosen for project..."
}
```

#### **Session Summary Log** (`session_summaries_*.log`)
```json
{
  "timestamp": "2026-02-04T11:38:06.123456",
  "session_id": "session-123",
  "session_summary": {
    "user_profile": {
      "prefs": ["speed", "flexibility"],
      "constraints": ["budget: limited", "team: 2 engineers"]
    },
    "key_facts": [
      "Building ML system for production",
      "Team has PyTorch experience"
    ],
    "decisions": [
      "Chose TensorFlow for deployment",
      "Using transfer learning approach"
    ],
    "open_questions": [
      "How to optimize training speed?"
    ]
  },
  "message_range_summarized": {
    "from": 0,
    "to": 42
  }
}
```

---

## Running Tests & Generating Logs

### ğŸ“ Test Scripts Location
```
tests/
â”œâ”€â”€ test_ambiguous_query_detection.py    â† Ambiguity detection + all 6 rules
â”œâ”€â”€ test_query_refinement.py              â† Query refinement with LLM
â”œâ”€â”€ test_session_summarization.py         â† Session memory & summarization
â”œâ”€â”€ test_conversation_logging.py          â† Conversation persistence
â”œâ”€â”€ test_cli_demo.py                      â† CLI interface testing
â”œâ”€â”€ test_streamlit_app.py                 â† Streamlit UI testing
â””â”€â”€ run_tests.py                          â† Run all tests
```

### ğŸš€ Running Tests & Generating Logs

#### **1. Test Ambiguity Detection (All 6 Rules)**
```bash
# Run: Tests all 6 ambiguity rules with natural conversation
python tests/test_ambiguous_query_detection.py

# Generates logs:
# logs/ambiguous_query_detection/
# â”œâ”€â”€ conversations_test.log      (user-assistant pairs with metadata)
# â”œâ”€â”€ user_queries_test.log        (original + rewritten queries, ambiguity flags)
# â””â”€â”€ session_summaries_test.log   (session memory evolution)

# Example output:
# [âœ“] Test: Ambiguous query detection
# [âœ“] Query 1: "We're building a machine learning system" â†’ CLEAR (100% confidence)
# [âœ“] Query 3: "How does it perform?" â†’ AMBIGUOUS (RULE 1: pronoun without antecedent)
# [âœ“] Query 7: "Which one do you prefer?" â†’ AMBIGUOUS (RULE 1c: which-one without context)
# [âœ“] Query 23: "It?" â†’ AMBIGUOUS (RULE 2: very short question)
# [âœ“] Overall: 24/28 queries correctly classified (85.7% accuracy)
```

#### **2. Test Query Refinement (Lightweight Model)**
```bash
# Run: Tests pronoun replacement with Qwen2.5-1.5B model
python tests/test_query_refinement.py

# Prerequisites: Ollama running with qwen2.5:1.5b pulled
# ollama pull qwen2.5:1.5b
# ollama serve

# Generates logs:
# logs/query_refinement/
# â”œâ”€â”€ conversations_test.log
# â”œâ”€â”€ user_queries_test.log        (shows rewritten_query field populated)
# â””â”€â”€ session_summaries_test.log

# Example output:
# [Original] "How does it perform?"
# [Refined]  "How does TensorFlow perform?" âœ“
# [Cache]    ["TensorFlow is fast", "PyTorch is flexible"]
```

#### **3. Test Session Summarization**
```bash
# Run: Tests token-aware summarization when context exceeds threshold
python tests/test_session_summarization.py

# Generates logs:
# logs/session_summarization/
# â”œâ”€â”€ conversations_test.log
# â”œâ”€â”€ user_queries_test.log
# â””â”€â”€ session_summaries_test.log  (shows summarization_triggered + summary content)

# Example output:
# Token count: 2500
# [âœ“] Summarization triggered at 10000 tokens
# [âœ“] Extracted 5 key facts
# [âœ“] Tracked 3 decisions
# [âœ“] Found 2 open questions
```

#### **4. Run All Tests**
```bash
# Run entire test suite
python tests/run_tests.py

# Generates all logs across test directories:
logs/
â”œâ”€â”€ ambiguous_query_detection/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log
â”œâ”€â”€ query_refinement/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log
â”œâ”€â”€ session_summarization/
â”‚   â”œâ”€â”€ conversations_test.log
â”‚   â”œâ”€â”€ user_queries_test.log
â”‚   â””â”€â”€ session_summaries_test.log
â””â”€â”€ ...
```

## Quick Start

### Prerequisites
- Python 3.10+
- Docker (for containerized deployment)
- API Keys: Google Generative AI (Gemini)

### Installation

1. **Clone and Setup**
```bash
git clone <repo-url>
cd chat-bot-with-memory
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. **Configure Environment**
```bash
cp .env.example .env
# Edit .env with your API keys:
# - GOOGLE_API_KEY
# - OLLAMA_HOST (optional, for fallback)
```

3. **Run Locally**

**Option A: CLI Demo**
```bash
python cli_demo.py
```

**Option B: Streamlit UI**
```bash
streamlit run streamlit_app.py
```

**Option C: FastAPI Backend**
```bash
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Docker Deployment (Local)

**API Service**
```bash
docker build -t chat-bot-api .
docker run -e PORT=8000 -e GOOGLE_API_KEY=your-key -p 8000:8000 chat-bot-api
```

**Streamlit UI**
```bash
docker build -f Dockerfile.streamlit -t chat-bot-ui .
docker run -e PORT=8501 -p 8501:8501 chat-bot-ui
```

## File Structure

```
chat-bot-with-memory/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ api/                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ docs.py          # Document upload (future)
â”‚   â”‚   â””â”€â”€ auth.py          # Authentication (future)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”‚   â””â”€â”€ token_counter.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py        # Base LLM client
â”‚   â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”‚   â””â”€â”€ json_guard.py    # JSON parsing utilities
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ session_store.py # Session persistence
â”‚   â”‚   â”œâ”€â”€ schemas.py       # Data models
â”‚   â”‚   â”œâ”€â”€ summarizer.py    # Token-aware summarization
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ query_understanding/
â”‚   â”‚   â”œâ”€â”€ ambiguity.py     # Detect ambiguous queries
â”‚   â”‚   â”œâ”€â”€ clarifier.py     # Generate clarifying questions
â”‚   â”‚   â”œâ”€â”€ context.py       # Context augmentation
â”‚   â”‚   â”œâ”€â”€ rewrite.py       # Query rewriting
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py       # ConversationLogger
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_session_summarization.py
â”‚   â”œâ”€â”€ test_ambiguous_query_detection.py
â”‚   â”œâ”€â”€ test_query_refinement.py
â”‚   â”œâ”€â”€ test_conversation_logging.py
â”‚   â”œâ”€â”€ test_cli_demo.py
â”‚   â”œâ”€â”€ test_streamlit_app.py
â”‚   â””â”€â”€ run_tests.py
â”œâ”€â”€ data/                    # Persistent data
â”‚   â”œâ”€â”€ sessions/            # Session summaries (JSON)
â”‚   â””â”€â”€ conversations/       # Conversation logs (JSONL)
â”œâ”€â”€ logs/                    # Application logs
â”œâ”€â”€ cli_demo.py              # CLI interface
â”œâ”€â”€ streamlit_app.py         # Streamlit web UI
â”œâ”€â”€ Dockerfile               # API service container
â”œâ”€â”€ Dockerfile.streamlit     # Streamlit UI container
```

## Environment Variables

```bash
# LLM Configuration
GOOGLE_API_KEY=your-gemini-api-key
OLLAMA_HOST=http://localhost:11434  # For fallback

# Session Management
SESSION_TOKEN_THRESHOLD=10000        # Summarize at this token count
SESSION_STORAGE_TYPE=file            # or 'redis'
REDIS_URL=redis://localhost:6379     # If using Redis
```

Run specific test:
```bash
pytest tests/test_session_summarization.py -v
```

**Session Management**
```bash
# View session info
python session_manager.py -i default_session

# Delete session
python session_manager.py -d default_session

# Delete all sessions
python session_manager.py -d
```

## License

MIT

## Support

For issues, questions, or feature requests, open a GitHub issue.

---

**Last Updated**: February 2026
**Current Version**: 1.0.0
**Status**: Production Ready (Core Features)
